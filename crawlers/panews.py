"""
PANews Crawler - è·å– PANews é‡è¦å¿«è®¯
https://www.panewslab.com/zh/newsflash
"""

import asyncio
import json
import hashlib
from datetime import datetime
from typing import Optional
from pathlib import Path

try:
    from playwright.async_api import async_playwright, Browser, Page
except ImportError:
    raise ImportError("è¯·å®‰è£… playwright: pip install playwright && playwright install chromium")


class PANewsCrawler:
    """PANews é‡è¦èµ„è®¯çˆ¬è™«"""
    
    BASE_URL = "https://www.panewslab.com/zh/newsflash"
    
    def __init__(self, headless: bool = True, cache_dir: Optional[str] = None):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            headless: æ˜¯å¦æ— å¤´æ¨¡å¼è¿è¡Œæµè§ˆå™¨
            cache_dir: ç¼“å­˜ç›®å½•ï¼Œç”¨äºå»é‡
        """
        self.headless = headless
        self.cache_dir = Path(cache_dir) if cache_dir else Path("./data/panews_cache")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self._seen_ids_file = self.cache_dir / "seen_ids.json"
        self._seen_ids: set = self._load_seen_ids()
    
    def _load_seen_ids(self) -> set:
        """åŠ è½½å·²çˆ¬å–çš„æ–°é—»ID"""
        if self._seen_ids_file.exists():
            with open(self._seen_ids_file, 'r') as f:
                return set(json.load(f))
        return set()
    
    def _save_seen_ids(self):
        """ä¿å­˜å·²çˆ¬å–çš„æ–°é—»ID"""
        with open(self._seen_ids_file, 'w') as f:
            json.dump(list(self._seen_ids), f)
    
    def _generate_id(self, title: str, time_str: str, link: str = '') -> str:
        """
        ç”Ÿæˆæ–°é—»å”¯ä¸€ID
        ä¼˜å…ˆä½¿ç”¨ link (æ–‡ç« URL) ä½œä¸ºå”¯ä¸€æ ‡è¯†
        """
        # ä¼˜å…ˆä½¿ç”¨ linkï¼Œè¿™æ˜¯æœ€å¯é çš„å”¯ä¸€æ ‡è¯†
        if link:
            # ä» link ä¸­æå–æ–‡ç«  ID
            # ä¾‹å¦‚: https://www.panewslab.com/zh/articles/abc123 -> abc123
            article_id = link.rstrip('/').split('/')[-1]
            if article_id and len(article_id) > 5:
                return hashlib.md5(article_id.encode()).hexdigest()[:12]
        
        # å¤‡ç”¨ï¼šä½¿ç”¨ title + time
        content = f"{title}_{time_str}"
        return hashlib.md5(content.encode()).hexdigest()[:12]
    
    async def _close_popups(self, page: Page):
        """å…³é—­å„ç§å¼¹çª—"""
        try:
            # å…³é—­ OneSignal è®¢é˜…å¼¹çª—
            cancel_btn = await page.query_selector('#onesignal-slidedown-cancel-button')
            if cancel_btn:
                await cancel_btn.click()
                await asyncio.sleep(0.5)
            
            # å…³é—­å…¬å‘Šå¼¹çª— (ç‚¹å‡»å…³é—­æŒ‰é’®æˆ–èƒŒæ™¯)
            close_btns = await page.query_selector_all('button[aria-label="close"], .close-btn, [class*="close"]')
            for btn in close_btns:
                try:
                    await btn.click()
                    await asyncio.sleep(0.3)
                except:
                    pass
        except Exception as e:
            print(f"å…³é—­å¼¹çª—æ—¶å‡ºé”™: {e}")
    
    async def _enable_important_filter(self, page: Page):
        """å¯ç”¨"åªçœ‹é‡è¦"ç­›é€‰"""
        try:
            # æ–¹æ³•1: ç›´æ¥ç‚¹å‡»ç­›é€‰æŒ‰é’® (æ ¹æ® DOM åˆ†æç»“æœ)
            result = await page.evaluate('''
                () => {
                    // æ–¹æ³•1: ä½¿ç”¨ button#v-0-0 (åªçœ‹é‡è¦æŒ‰é’®)
                    const filterBtn = document.querySelector('button#v-0-0');
                    if (filterBtn) {
                        filterBtn.click();
                        return "button_clicked";
                    }
                    
                    // æ–¹æ³•2: æŸ¥æ‰¾åŒ…å«"åªçœ‹é‡è¦"æ–‡æœ¬çš„å…ƒç´ 
                    const elements = document.querySelectorAll('label, span, div, button');
                    for (const el of elements) {
                        if (el.textContent?.trim() === 'åªçœ‹é‡è¦') {
                            el.click();
                            return "text_clicked";
                        }
                    }
                    
                    // æ–¹æ³•3: æŸ¥æ‰¾ checkbox
                    const checkboxes = document.querySelectorAll('input[type="checkbox"]');
                    for (const cb of checkboxes) {
                        const label = cb.closest('label') || cb.nextElementSibling;
                        if (label?.textContent?.includes('åªçœ‹é‡è¦')) {
                            if (!cb.checked) cb.click();
                            return "checkbox_clicked";
                        }
                    }
                    
                    return "not_found";
                }
            ''')
            print(f"ç­›é€‰å™¨çŠ¶æ€: {result}")
            await asyncio.sleep(2)  # ç­‰å¾…ç­›é€‰ç”Ÿæ•ˆ
        except Exception as e:
            print(f"å¯ç”¨ç­›é€‰å™¨å¤±è´¥: {e}")
    
    async def _extract_news(self, page: Page) -> list[dict]:
        """ä»é¡µé¢æå–æ–°é—»åˆ—è¡¨ - åªæŠ“å–æœ‰æ—¶é—´çš„å¿«è®¯ï¼ŒæŒ‰æ—¥æœŸ+æ—¶é—´æ’åº"""
        news_list = await page.evaluate(r'''
            () => {
                const results = [];
                const timeRegex = /^\d{1,2}:\d{2}$/;
                
                // æ‰¾åˆ°æ‰€æœ‰æ—¶é—´å…ƒç´ 
                for (const el of document.querySelectorAll('*')) {
                    if (el.children.length > 0) continue;
                    const timeText = el.textContent?.trim();
                    if (!timeText || !timeRegex.test(timeText)) continue;
                    if (el.closest('aside') || el.closest('nav') || el.closest('[class*="sidebar"]')) continue;
                    
                    const time = timeText;
                    
                    // å‘ä¸ŠæŸ¥æ‰¾æ–°é—»å†…å®¹
                    let container = el.parentElement;
                    for (let i = 0; i < 5 && container; i++) {
                        const link = container.querySelector('a[href*="/articles/"], a[href*="/newsflash/"]');
                        if (link) {
                            const title = link.textContent?.trim();
                            const href = link.href || link.getAttribute('href') || '';
                            
                            if (!title || title.length < 5) {
                                container = container.parentElement;
                                continue;
                            }
                            
                            const descEl = container.querySelector('div.text-neutrals-60, div.line-clamp-3, div.line-clamp-2, p');
                            let description = descEl?.textContent?.trim() || '';
                            if (description.startsWith(title)) {
                                description = description.slice(title.length).trim();
                            }
                            
                            // ä»æ‘˜è¦ä¸­æå–æ—¥æœŸ (PANews 1æœˆ5æ—¥æ¶ˆæ¯ -> 1æœˆ5æ—¥)
                            let dateNum = 0;  // ç”¨äºæ’åºçš„æ—¥æœŸæ•°å€¼
                            const dateMatch = description.match(/(\d{1,2})æœˆ(\d{1,2})æ—¥/);
                            if (dateMatch) {
                                const month = parseInt(dateMatch[1]);
                                const day = parseInt(dateMatch[2]);
                                dateNum = month * 100 + day;  // 105 = 1æœˆ5æ—¥
                            }
                            
                            const hasImportantTag = container.querySelector('span.bg-brand-primary, [class*="tag"]') !== null;
                            
                            if (results.some(r => r.link === href)) break;
                            
                            const [h, m] = time.split(':').map(Number);
                            
                            results.push({
                                time,
                                title,
                                content: description,
                                link: href,
                                isImportant: hasImportantTag,
                                _dateNum: dateNum,
                                _timeNum: h * 60 + m
                            });
                            break;
                        }
                        container = container.parentElement;
                    }
                }
                
                // æ’åºï¼šæ—¥æœŸé™åºï¼ˆå¤§çš„åœ¨å‰ï¼‰ï¼ŒåŒæ—¥æ—¶é—´é™åº
                results.sort((a, b) => {
                    if (a._dateNum !== b._dateNum) {
                        return b._dateNum - a._dateNum;  // æ—¥æœŸå¤§çš„åœ¨å‰ (1æœˆ5æ—¥ > 1æœˆ4æ—¥)
                    }
                    return b._timeNum - a._timeNum;  // æ—¶é—´å¤§çš„åœ¨å‰ (12:45 > 11:00)
                });
                
                // ç§»é™¤æ’åºé”®
                results.forEach(r => {
                    delete r._dateNum;
                    delete r._timeNum;
                });
                
                return results;
            }
        ''')
        return news_list
    
    async def fetch_important_news(self, only_new: bool = True, save_to_db: bool = True) -> list[dict]:
        """
        è·å–é‡è¦å¿«è®¯
        
        Args:
            only_new: æ˜¯å¦åªè¿”å›æ–°çš„ï¼ˆæœªè§è¿‡çš„ï¼‰æ–°é—»
            save_to_db: æ˜¯å¦ä¿å­˜åˆ°æ•°æ®åº“
            
        Returns:
            æ–°é—»åˆ—è¡¨ï¼Œæ¯æ¡åŒ…å« time, title, content, link, id
        """
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=self.headless)
            context = await browser.new_context(
                viewport={'width': 1280, 'height': 800},
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            )
            page = await context.new_page()
            
            try:
                print(f"æ­£åœ¨è®¿é—® {self.BASE_URL}...")
                await page.goto(self.BASE_URL, wait_until='domcontentloaded', timeout=60000)
                await asyncio.sleep(5)  # ç­‰å¾…JSæ¸²æŸ“å®Œæˆ
                
                # å…³é—­å¼¹çª—
                await self._close_popups(page)
                
                # å¯ç”¨"åªçœ‹é‡è¦"ç­›é€‰
                await self._enable_important_filter(page)
                
                # ç­‰å¾…å†…å®¹åŠ è½½
                await asyncio.sleep(2)
                
                # æå–æ–°é—»
                news_list = await self._extract_news(page)
                print(f"å…±æŠ“å–åˆ° {len(news_list)} æ¡èµ„è®¯")
                
                # å¤„ç†ç»“æœ
                results = []
                for news in news_list:
                    news_id = self._generate_id(news['title'], news.get('time', ''), news.get('link', ''))
                    
                    if only_new and news_id in self._seen_ids:
                        continue
                    
                    news['id'] = news_id
                    news['crawled_at'] = datetime.now().isoformat()
                    news['source'] = 'PANews'
                    results.append(news)
                    
                    self._seen_ids.add(news_id)
                
                # ä¿å­˜å·²è§ID
                self._save_seen_ids()
                
                # ä¿å­˜åˆ°æ•°æ®åº“
                if save_to_db and results:
                    from .storage import get_storage
                    storage = get_storage()
                    inserted = storage.save_news(results)
                    print(f"ğŸ’¾ ä¿å­˜ {inserted} æ¡æ–°èµ„è®¯åˆ°æ•°æ®åº“")
                    # æ¸…ç†è¿‡æœŸæ•°æ®
                    storage.cleanup_expired()
                
                print(f"å…¶ä¸­ {len(results)} æ¡ä¸ºæ–°èµ„è®¯")
                return results
                
            except Exception as e:
                print(f"çˆ¬å–å¤±è´¥: {e}")
                raise
            finally:
                await browser.close()
    
    def fetch_sync(self, only_new: bool = True, save_to_db: bool = True) -> list[dict]:
        """
        åŒæ­¥ç‰ˆæœ¬çš„è·å–æ–¹æ³•
        
        Args:
            only_new: åªè¿”å›æ–°èµ„è®¯
            save_to_db: æ˜¯å¦ä¿å­˜åˆ°æ•°æ®åº“
        """
        return asyncio.run(self.fetch_important_news(only_new, save_to_db))


# å‘½ä»¤è¡Œæµ‹è¯•
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='PANews é‡è¦èµ„è®¯çˆ¬è™«')
    parser.add_argument('--no-headless', action='store_true', help='æ˜¾ç¤ºæµè§ˆå™¨çª—å£')
    parser.add_argument('--all', action='store_true', help='è·å–æ‰€æœ‰èµ„è®¯ï¼Œä¸å»é‡')
    args = parser.parse_args()
    
    crawler = PANewsCrawler(headless=not args.no_headless)
    news = crawler.fetch_sync(only_new=not args.all)
    
    print("\n" + "="*60)
    print(f"è·å–åˆ° {len(news)} æ¡é‡è¦èµ„è®¯:")
    print("="*60)
    
    for item in news:
        print(f"\nâ° {item['time']}")
        print(f"ğŸ“° {item['title']}")
        if item['content']:
            print(f"   {item['content'][:100]}...")
        print(f"ğŸ”— {item['link']}")
