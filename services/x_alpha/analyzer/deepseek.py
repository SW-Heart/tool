"""
DeepSeek AI åˆ†æå™¨
ä½¿ç”¨ DeepSeek API åˆ†ææ¨æ–‡ï¼Œæå–é‡‘èä¿¡å·
"""
import asyncio
import json
from typing import List, Dict, Any, Optional
from openai import AsyncOpenAI
import sys
from pathlib import Path

# æ·»åŠ è·¯å¾„ä»¥æ”¯æŒå¯¼å…¥
CURRENT_DIR = Path(__file__).parent  # analyzer
SERVICE_DIR = CURRENT_DIR.parent     # x_alpha
ROOT_DIR = SERVICE_DIR.parent.parent # tool

# ä¼˜å…ˆåŠ è½½æœåŠ¡ç›®å½• (x_alpha) ä»¥åŒ¹é… config
sys.path.insert(0, str(SERVICE_DIR))
# åŠ è½½æ ¹ç›®å½•ä»¥æ”¯æŒ shared
sys.path.insert(1, str(ROOT_DIR))

from shared.logger import setup_logger

logger = setup_logger("x_alpha.analyzer")

# System Prompt for DeepSeek
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªé‡‘èæƒ…æŠ¥åˆ†æå¸ˆã€‚åˆ†æç”¨æˆ·è¾“å…¥çš„æ¨æ–‡ã€‚

åˆ¤æ–­è§„åˆ™ï¼š
1. å¦‚æœå†…å®¹ä¸åŠ å¯†è´§å¸ã€è‚¡ç¥¨ã€å®è§‚ç»æµæ— å…³ï¼Œæˆ–ä»…ä¸ºé—²èŠ/è¡¨æƒ…åŒ…/æ—¥å¸¸ç”Ÿæ´»ï¼Œæ ‡è®°ä¸º irrelevant
2. å¦‚æœç›¸å…³ï¼Œæå–äº¤æ˜“ä¿¡å·

ä¿¡å·è¯„åˆ†æ ‡å‡† (sentiment_score 0-10):
- 0-2: æåº¦æ‚²è§‚/çœ‹è·Œ
- 3-4: åç©º
- 5: ä¸­æ€§
- 6-7: åå¤š
- 8-10: æåº¦ä¹è§‚/çœ‹æ¶¨

ä¿¡å·ç±»å‹ (signal_type):
- BUY: æ˜ç¡®çš„ä¹°å…¥ä¿¡å·æˆ–æåº¦çœ‹æ¶¨è¨€è®º
- SELL: æ˜ç¡®çš„å–å‡ºä¿¡å·æˆ–æåº¦çœ‹è·Œè¨€è®º
- WATCH: éœ€è¦å…³æ³¨ä½†ä¸æ„æˆäº¤æ˜“ä¿¡å·çš„é‡è¦ä¿¡æ¯
- NEUTRAL: ä¸­æ€§æˆ–ä¿¡æ¯é‡ä¸è¶³

å¿…é¡»è¾“å‡ºçº¯ JSON æ ¼å¼ï¼Œä¸è¦åŒ…å« Markdown æ ‡è®°ã€ä»£ç å—æˆ–ä»»ä½•å…¶ä»–æ–‡å­—ã€‚
"""

OUTPUT_FORMAT = """{
  "is_relevant": trueæˆ–false,
  "sentiment_score": 0-10çš„æ•´æ•°,
  "related_assets": ["BTC", "ETH"]æˆ–å…¶ä»–ç›¸å…³èµ„äº§ä»£ç åˆ—è¡¨,
  "signal_type": "BUY"æˆ–"SELL"æˆ–"WATCH"æˆ–"NEUTRAL",
  "summary_zh": "ä¸­æ–‡ä¸€å¥è¯æ‘˜è¦ï¼ˆ15-30å­—ï¼‰"
}"""


class DeepSeekAnalyzer:
    """
    ä½¿ç”¨ DeepSeek API åˆ†ææ¨æ–‡
    """
    
    def __init__(
        self,
        api_key: str,
        base_url: str = "https://api.deepseek.com",
        model: str = "deepseek-chat",
        timeout: int = 60,
        max_retries: int = 3,
    ):
        self.client = AsyncOpenAI(
            api_key=api_key,
            base_url=base_url,
            timeout=timeout,
        )
        self.model = model
        self.max_retries = max_retries
    
    async def analyze_tweet(self, tweet: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆ†æå•æ¡æ¨æ–‡
        
        Args:
            tweet: æ¨æ–‡æ•°æ® {"author": "xxx", "content": "xxx", ...}
            
        Returns:
            åˆ†æç»“æœ
        """
        author = tweet.get("author", "unknown")
        content = tweet.get("content", "")
        
        if not content or len(content.strip()) < 5:
            return self._default_result(irrelevant=True)
        
        user_prompt = f"KOL: @{author}\næ¨æ–‡å†…å®¹: {content}\n\nè¯·åˆ†æå¹¶è¿”å› JSON:"
        
        for attempt in range(self.max_retries):
            try:
                response = await self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": f"{SYSTEM_PROMPT}\n\nè¾“å‡ºæ ¼å¼:\n{OUTPUT_FORMAT}"},
                        {"role": "user", "content": user_prompt},
                    ],
                    temperature=0.3,
                    max_tokens=500,
                )
                
                result_text = response.choices[0].message.content.strip()
                result = self._parse_result(result_text)
                
                # [æ–°å¢] å°†æ¨æ–‡é“¾æ¥è¿½åŠ åˆ°æ‘˜è¦æœ«å°¾ï¼Œå¢åŠ å¯ä¿¡åº¦
                if tweet.get("tweet_url"):
                    summary = result.get("summary_zh", "")
                    # é¿å…é‡å¤æ·»åŠ 
                    if tweet["tweet_url"] not in summary:
                        result["summary_zh"] = f"{summary} {tweet['tweet_url']}"
                
                logger.debug(f"åˆ†æå®Œæˆ [{author}]: {result.get('signal_type')} / {result.get('sentiment_score')}")
                return result
                
            except json.JSONDecodeError as e:
                logger.warning(f"JSON è§£æå¤±è´¥ (å°è¯• {attempt + 1}): {e}")
            except Exception as e:
                logger.error(f"åˆ†æå¤±è´¥ (å°è¯• {attempt + 1}): {e}")
            
            if attempt < self.max_retries - 1:
                await asyncio.sleep(2)
        
        return self._default_result(irrelevant=True)
    
    def _parse_result(self, text: str) -> Dict[str, Any]:
        """è§£æ AI è¿”å›çš„ JSON ç»“æœ"""
        # ç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
        text = text.strip()
        if text.startswith("```"):
            lines = text.split("\n")
            text = "\n".join(lines[1:-1] if lines[-1].strip() == "```" else lines[1:])
        
        # å°è¯•è§£æ JSON
        try:
            result = json.loads(text)
        except json.JSONDecodeError:
            # å°è¯•æå– JSON éƒ¨åˆ†
            import re
            json_match = re.search(r'\{[^{}]*\}', text, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
            else:
                raise
        
        # éªŒè¯å’Œè§„èŒƒåŒ–å­—æ®µ
        return {
            "is_relevant": bool(result.get("is_relevant", False)),
            "sentiment_score": max(0, min(10, int(result.get("sentiment_score", 5)))),
            "related_assets": result.get("related_assets", []) or [],
            "signal_type": result.get("signal_type", "NEUTRAL"),
            "summary_zh": result.get("summary_zh", ""),
        }
    
    def _default_result(self, irrelevant: bool = False) -> Dict[str, Any]:
        """è¿”å›é»˜è®¤åˆ†æç»“æœ"""
        return {
            "is_relevant": not irrelevant,
            "sentiment_score": 5,
            "related_assets": [],
            "signal_type": "NEUTRAL",
            "summary_zh": "",
        }
    
    async def batch_analyze(
        self,
        tweets: List[Dict[str, Any]],
        concurrency: int = 5,
    ) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡åˆ†ææ¨æ–‡
        
        Args:
            tweets: æ¨æ–‡åˆ—è¡¨
            concurrency: å¹¶å‘æ•°é‡
            
        Returns:
            åˆ†æç»“æœåˆ—è¡¨ (ä¸è¾“å…¥é¡ºåºå¯¹åº”)
        """
        semaphore = asyncio.Semaphore(concurrency)
        
        async def analyze_with_semaphore(tweet: Dict[str, Any]) -> Dict[str, Any]:
            async with semaphore:
                result = await self.analyze_tweet(tweet)
                # åˆå¹¶æ¨æ–‡åŸå§‹æ•°æ®å’Œåˆ†æç»“æœ
                return {**tweet, **result}
        
        tasks = [analyze_with_semaphore(tweet) for tweet in tweets]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†å¼‚å¸¸
        final_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"æ‰¹é‡åˆ†æå¼‚å¸¸: {result}")
                final_results.append({**tweets[i], **self._default_result(irrelevant=True)})
            else:
                final_results.append(result)
        
        relevant_count = sum(1 for r in final_results if r.get("is_relevant"))
        logger.info(f"æ‰¹é‡åˆ†æå®Œæˆ: {len(tweets)} æ¡æ¨æ–‡, {relevant_count} æ¡ç›¸å…³")
        
        return final_results


# æµ‹è¯•ä»£ç 
async def test_analyzer():
    from config import DEEPSEEK_CONFIG
    
    analyzer = DeepSeekAnalyzer(
        api_key=DEEPSEEK_CONFIG["api_key"],
        base_url=DEEPSEEK_CONFIG["base_url"],
    )
    
    test_tweet = {
        "author": "elonmusk",
        "content": "Bitcoin is the future of money. HODL strong! ğŸš€",
        "tweet_url": "https://x.com/elonmusk/status/123456789",
    }
    
    result = await analyzer.analyze_tweet(test_tweet)
    print(json.dumps(result, ensure_ascii=False, indent=2))


if __name__ == "__main__":
    asyncio.run(test_analyzer())
