"""
PANews Crawler - è·å– PANews é‡è¦å¿«è®¯
https://www.panewslab.com/zh/newsflash
"""

import asyncio
import json
import hashlib
from datetime import datetime
from typing import Optional
from pathlib import Path

try:
    from playwright.async_api import async_playwright, Browser, Page, Locator
except ImportError:
    raise ImportError("è¯·å®‰è£… playwright: pip install playwright && playwright install chromium")


class PANewsCrawler:
    """PANews é‡è¦èµ„è®¯çˆ¬è™«"""
    
    BASE_URL = "https://www.panewslab.com/zh/newsflash"
    
    # Selectors
    SEL_NEWS_ITEM_CONTAINER = '.news-item-container, .panews-flash-item'  # Generic fallback
    SEL_IMPORTANT_FILTER_BTN = 'text="åªçœ‹é‡è¦"'
    SEL_POPUP_CLOSE_BTN = 'button[aria-label="close"], .close-btn, [class*="close"]'
    SEL_ONESIGNAL_CANCEL = '#onesignal-slidedown-cancel-button'
    
    def __init__(self, headless: bool = True, cache_dir: Optional[str] = None):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            headless: æ˜¯å¦æ— å¤´æ¨¡å¼è¿è¡Œæµè§ˆå™¨
            cache_dir: ç¼“å­˜ç›®å½•ï¼Œç”¨äºå»é‡
        """
        self.headless = headless
        self.cache_dir = Path(cache_dir) if cache_dir else Path("./data/panews_cache")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self._seen_ids_file = self.cache_dir / "seen_ids.json"
        self._seen_ids: set = self._load_seen_ids()
    
    def _load_seen_ids(self) -> set:
        """åŠ è½½å·²çˆ¬å–çš„æ–°é—»ID"""
        if self._seen_ids_file.exists():
            with open(self._seen_ids_file, 'r') as f:
                return set(json.load(f))
        return set()
    
    def _save_seen_ids(self):
        """ä¿å­˜å·²çˆ¬å–çš„æ–°é—»ID"""
        with open(self._seen_ids_file, 'w') as f:
            json.dump(list(self._seen_ids), f)
    
    def _generate_id(self, title: str, time_str: str, link: str = '') -> str:
        """
        ç”Ÿæˆæ–°é—»å”¯ä¸€ID
        ä¼˜å…ˆä½¿ç”¨ link (æ–‡ç« URL) ä½œä¸ºå”¯ä¸€æ ‡è¯†
        """
        # ä¼˜å…ˆä½¿ç”¨ linkï¼Œè¿™æ˜¯æœ€å¯é çš„å”¯ä¸€æ ‡è¯†
        if link:
            # ä» link ä¸­æå–æ–‡ç«  ID
            # ä¾‹å¦‚: https://www.panewslab.com/zh/articles/abc123 -> abc123
            article_id = link.rstrip('/').split('/')[-1]
            if article_id and len(article_id) > 5:
                # Remove query params if any
                article_id = article_id.split('?')[0]
                return hashlib.md5(article_id.encode()).hexdigest()[:12]
        
        # å¤‡ç”¨ï¼šä½¿ç”¨ title + time
        content = f"{title}_{time_str}"
        return hashlib.md5(content.encode()).hexdigest()[:12]
    
    async def _close_popups(self, page: Page):
        """å…³é—­å„ç§å¼¹çª— (å¸¦è¶…æ—¶ä¿æŠ¤)"""
        try:
            # 1. OneSignal
            try:
                # Using locator instead of query_selector for auto-waiting if needed, 
                # but for popups we usually want short timeout.
                os_btn = page.locator(self.SEL_ONESIGNAL_CANCEL)
                if await os_btn.is_visible(timeout=2000):
                    await os_btn.click()
            except Exception:
                pass

            # 2. Generic Popups
            # Try finding close buttons
            for _ in range(3): # Retry a few times quickly
                try:
                    # Look for common close buttons
                    close_btn = page.locator(self.SEL_POPUP_CLOSE_BTN).first
                    if await close_btn.is_visible(timeout=1000):
                        await close_btn.click()
                        await asyncio.sleep(0.5)
                    else:
                        break
                except Exception:
                    break
        except Exception as e:
            print(f"å…³é—­å¼¹çª—æ—¶è­¦å‘Š: {e}")
    
    async def _enable_important_filter(self, page: Page):
        """å¯ç”¨"åªçœ‹é‡è¦"ç­›é€‰"""
        print("å°è¯•ç‚¹å‡» 'åªçœ‹é‡è¦'...")
        try:
            # ä½¿ç”¨ Playwright çš„ text selectorï¼Œéå¸¸å¥å£®
            # We wait a bit longer here because the filter button might render late
            filter_btn = page.locator(self.SEL_IMPORTANT_FILTER_BTN).first
            
            # Check if already active? Hard to tell without specific class. 
            # Usually clicking it enables it.
            
            if await filter_btn.is_visible(timeout=5000):
                await filter_btn.click()
                print("å·²ç‚¹å‡»ç­›é€‰æŒ‰é’®")
                # Wait for list to update - hard to detect, just wait a bit or wait for network idle
                await page.wait_for_load_state("networkidle", timeout=3000)
                await asyncio.sleep(1.0) 
            else:
                print("âš ï¸ æœªæ‰¾åˆ° 'åªçœ‹é‡è¦' æŒ‰é’®ï¼Œå¯èƒ½å·²æ”¹ç‰ˆæˆ–é»˜è®¤å·²é€‰")
                
                # Fallback: Try button id "v-0-0" seen in old code
                fallback_btn = page.locator("button#v-0-0")
                if await fallback_btn.is_visible(timeout=2000):
                     await fallback_btn.click()
                     print("å·²ç‚¹å‡» fallback ç­›é€‰æŒ‰é’®")

        except Exception as e:
            print(f"å¯ç”¨ç­›é€‰å™¨å¤±è´¥: {e}")
    
    async def _extract_news(self, page: Page) -> list[dict]:
        """ä»é¡µé¢æå–æ–°é—»åˆ—è¡¨ - åªæŠ“å–æœ‰æ—¶é—´çš„å¿«è®¯ï¼ŒæŒ‰æ—¥æœŸ+æ—¶é—´æ’åº"""
        # We define the evaluation script separately for cleanliness
        # This script runs in the browser context
        extract_script = r'''
            () => {
                const results = [];
                const timeRegex = /^\d{1,2}:\d{2}$/;
                
                // Helper to finding the news container
                // PANews structure usually: ... -> div.item -> [ time, content... ]
                // We scan for time elements as anchors
                
                const allElements = document.querySelectorAll('*');
                
                for (const el of allElements) {
                    // Optimization: Skip container elements immediately
                    if (el.tagName === 'DIV' || el.tagName === 'SECTION' || el.tagName === 'MAIN') {
                        if (el.children.length > 5) continue; // heuristic
                    }
                    if (el.children.length > 1) continue; // leaf nodes or close to leaf

                    const text = el.textContent?.trim();
                    if (!text || !timeRegex.test(text)) continue;
                    
                    // Exclude sidebars
                    if (el.closest('aside') || el.closest('nav') || el.closest('.footer')) continue;
                    
                    const timeStr = text;
                    
                    // Found a time string (e.g. "14:24"). logic triggers.
                    // Walk up to find the container
                    let container = el.parentElement;
                    let foundNews = false;
                    
                    for (let i = 0; i < 6 && container; i++) {
                        // Look for links inside this container
                        const linkEl = container.querySelector('a[href*="/newsflash/"], a[href*="/articles/"]');
                        if (!linkEl) {
                            container = container.parentElement;
                            continue;
                        }

                        const title = linkEl.textContent?.trim();
                        if (!title || title.length < 2) {
                             container = container.parentElement;
                             continue;
                        }
                        
                        const href = linkEl.href;
                        
                        // Extract content/desc
                        // Heuristic: sibling of title, or inside container but not title/time
                        // Often text-neutrals-60 or similar
                        let content = "";
                        const contentEl = container.querySelector('.line-clamp-3, .line-clamp-2, p, [class*="content"]');
                        if (contentEl && contentEl !== linkEl) {
                            content = contentEl.textContent?.trim() || "";
                        }
                        
                        // Clean content if it starts with title
                        if (content.startsWith(title)) {
                            content = content.slice(title.length).trim();
                        }

                        // Determine Date
                        // Try to find date in the text (e.g. description often starts with "PANews 1æœˆ16æ—¥æ¶ˆæ¯")
                        let dateMatch = content.match(/(\d{1,2})æœˆ(\d{1,2})æ—¥/);
                        if (!dateMatch) {
                            // Try container text
                            dateMatch = container.textContent.match(/(\d{1,2})æœˆ(\d{1,2})æ—¥/);
                        }
                        
                        const now = new Date();
                        let year = now.getFullYear();
                        let month = now.getMonth() + 1;
                        let day = now.getDate();
                        
                        if (dateMatch) {
                            month = parseInt(dateMatch[1]);
                            day = parseInt(dateMatch[2]);
                            
                            // Year transition logic
                            // If news month is 12 and current month is 1, assume last year
                            // Or more generally, if news date is "in the future" by more than a day, it's likely last year
                            const currentTs = now.getTime();
                            const newsDateCurrentYear = new Date(year, month - 1, day);
                            
                            // 30 days buffer for safe check (e.g. clock skew or timezone)
                            // If news date (current year) is > now + 2 days, it's probably last year
                            if (newsDateCurrentYear.getTime() > currentTs + 86400000 * 2) {
                                year -= 1;
                            }
                        }
                        
                        const fullDateTime = `${year}-${String(month).padStart(2,'0')}-${String(day).padStart(2,'0')} ${timeStr}`;
                        
                        // Check exact important tag
                        const isImportant = container.querySelector('.bg-brand-primary, [class*="important"]') !== null || 
                                          (container.textContent && container.textContent.includes('é‡è¦')); 

                        // Avoid duplicates in this batch
                        if (!results.some(r => r.link === href)) {
                            results.push({
                                time: timeStr,
                                title: title,
                                content: content,
                                link: href,
                                isImportant: isImportant,
                                publishDateTime: fullDateTime
                            });
                        }
                        
                        foundNews = true;
                        break; // Found for this time element
                    }
                    if (foundNews) continue;
                }
                return results;
            }
        '''
        try:
            # Wait for content to actually be there specifically
            # We look for something that looks like news content
            await page.wait_for_selector('a[href*="/newsflash/"]', timeout=5000)
        except:
            print("âš ï¸è¶…æ—¶: é¡µé¢å¯èƒ½æœªåŠ è½½å®Œå…¨")

        news_list = await page.evaluate(extract_script)
        return news_list
    
    async def fetch_important_news(self, only_new: bool = True, save_to_db: bool = True, timeout: int = 300) -> list[dict]:
        """
        è·å–é‡è¦å¿«è®¯ (å¸¦è¶…æ—¶ä¿æŠ¤)
        
        Args:
            only_new: æ˜¯å¦åªè¿”å›æ–°çš„ï¼ˆæœªè§è¿‡çš„ï¼‰æ–°é—»
            save_to_db: æ˜¯å¦ä¿å­˜åˆ°æ•°æ®åº“
            timeout: æœ€å¤§æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤5åˆ†é’Ÿ
            
        Returns:
            æ–°é—»åˆ—è¡¨ï¼Œæ¯æ¡åŒ…å« time, title, content, link, id
        """
        try:
            return await asyncio.wait_for(
                self._fetch_important_news_impl(only_new, save_to_db),
                timeout=timeout
            )
        except asyncio.TimeoutError:
            print(f"âš ï¸ çˆ¬å–è¶…æ—¶ (è¶…è¿‡ {timeout} ç§’)ï¼Œå¼ºåˆ¶ç»ˆæ­¢")
            return []
        except Exception as e:
            print(f"âŒ çˆ¬å–è¿‡ç¨‹å‡ºé”™: {e}")
            return []
    
    async def _fetch_important_news_impl(self, only_new: bool = True, save_to_db: bool = True) -> list[dict]:
        """å®é™…æ‰§è¡Œçˆ¬å–æ“ä½œçš„å†…éƒ¨æ–¹æ³•"""
        browser = None
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(
                    headless=self.headless,
                    args=['--disable-blink-features=AutomationControlled'] # é˜²æ­¢è¢«æ£€æµ‹
                )
                context = await browser.new_context(
                    viewport={'width': 1280, 'height': 800},
                    user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                )
                page = await context.new_page()
                
                print(f"æ­£åœ¨è®¿é—® {self.BASE_URL}...")
                response = await page.goto(self.BASE_URL, wait_until='domcontentloaded', timeout=60000)
                if not response:
                    print("âŒ æ— æ³•åŠ è½½é¡µé¢ (Response is None)")
                    return []
                    
                # ç­‰å¾…åŸºæœ¬çš„å¿«è®¯å…ƒç´ å‡ºç°ï¼Œè€Œä¸æ˜¯æ­»ç­‰sleep
                try:
                    await page.wait_for_selector('.list-content, .news-list, body', state='visible', timeout=10000)
                except:
                    pass

                # å…³é—­å¼¹çª—
                await self._close_popups(page)
                
                # å¯ç”¨"åªçœ‹é‡è¦"ç­›é€‰
                await self._enable_important_filter(page)
                
                # å†æ¬¡ç­‰å¾…ï¼Œç¡®ä¿åˆ—è¡¨åˆ·æ–°
                await asyncio.sleep(1.0) # Small buffer
                
                # æå–æ–°é—»
                news_list = await self._extract_news(page)
                print(f"å…±æŠ“å–åˆ° {len(news_list)} æ¡èµ„è®¯")
                
                # å¤„ç†ç»“æœ
                results = []
                for news in news_list:
                    news_id = self._generate_id(news['title'], news.get('time', ''), news.get('link', ''))
                    
                    if only_new and news_id in self._seen_ids:
                        continue
                    
                    news['id'] = news_id
                    news['crawled_at'] = datetime.now().isoformat()
                    news['source'] = 'PANews'
                    results.append(news)
                    
                    self._seen_ids.add(news_id)
                
                # ä¿å­˜å·²è§ID
                self._save_seen_ids()
                
                # ä¿å­˜åˆ°æ•°æ®åº“
                if save_to_db and results:
                    from .storage import get_storage
                    storage = get_storage()
                    inserted = storage.save_news(results)
                    print(f"ğŸ’¾ ä¿å­˜ {inserted} æ¡æ–°èµ„è®¯åˆ°æ•°æ®åº“")
                    # æ¸…ç†è¿‡æœŸæ•°æ®
                    storage.cleanup_expired()
                
                print(f"å…¶ä¸­ {len(results)} æ¡ä¸ºæ–°èµ„è®¯")
                return results
                
        except asyncio.CancelledError:
            print("âš ï¸ çˆ¬å–ä»»åŠ¡è¢«å–æ¶ˆ")
            raise
        except Exception as e:
            print(f"çˆ¬å–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise
        finally:
            # ç¡®ä¿æµè§ˆå™¨è¢«å…³é—­
            if browser:
                try:
                    await browser.close()
                    print("ğŸ§¹ æµè§ˆå™¨å·²å…³é—­")
                except Exception:
                    pass
    
    def fetch_sync(self, only_new: bool = True, save_to_db: bool = True) -> list[dict]:
        """
        åŒæ­¥ç‰ˆæœ¬çš„è·å–æ–¹æ³•
        
        Args:
            only_new: åªè¿”å›æ–°èµ„è®¯
            save_to_db: æ˜¯å¦ä¿å­˜åˆ°æ•°æ®åº“
        """
        return asyncio.run(self.fetch_important_news(only_new, save_to_db))


# å‘½ä»¤è¡Œæµ‹è¯•
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='PANews é‡è¦èµ„è®¯çˆ¬è™«')
    parser.add_argument('--no-headless', action='store_true', help='æ˜¾ç¤ºæµè§ˆå™¨çª—å£')
    parser.add_argument('--all', action='store_true', help='è·å–æ‰€æœ‰èµ„è®¯ï¼Œä¸å»é‡')
    args = parser.parse_args()
    
    crawler = PANewsCrawler(headless=not args.no_headless)
    news = crawler.fetch_sync(only_new=not args.all)
    
    print("\n" + "="*60)
    print(f"è·å–åˆ° {len(news)} æ¡é‡è¦èµ„è®¯:")
    print("="*60)
    
    for item in news:
        print(f"\nâ° {item['publishDateTime']}")
        print(f"ğŸ“° {item['title']}")
        if item.get('content'):
            print(f"   {item['content'][:100]}...")
        print(f"ğŸ”— {item['link']}")
